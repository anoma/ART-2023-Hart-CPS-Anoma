\section{Distributed CSPs}\label{sec:distributed-csps}

Rather than designing a central system to solve a CSP, it may be prudent to distribute the workload across several solvers. The idea is that we have numerous agents working in parallel, each responsible for a finite segment of the CSP problem (generally, a small set of variables or a single variable). They repeatedly use knowledge of their local environment to search for solutions and continuously send messages to locally relevant agents in order to approach a cooperative maximum.

The following quote from \citep{rossi2006handbook} should be noted as the main guiding principle to determine if distributed CSP methods are appropriate;

\begin{quote}
    In general, distributed techniques work well only when the problem is sparse and loose, i.e. each variable has constraints with only a few other variables, and for each constraint, there are many value combinations that satisfy it. When problems are dense and tight, usually the distributed solution requires so much information exchange among the agents that it would be much more efficient to communicate the problem to a central solver.
\end{quote}

For a broad overview of the subject, see \citep{fioretto2018distributed}, especially sections 4.3 and 4.4, which summarize algorithms and trade-offs in more detail than here.

\subsection{Complete Algorithms}\label{sec:complete-algorithms}

The most basic algorithms simply distribute search over all the agents. Each agent is assigned a variable, and they synchronously "count" through variable combinations. This requires the agents to be linearly ordered so that higher priority agents only change their assignment after the lower priority ones exhaust their options. This will clearly require an exponential number of messages to be passed between the agents.

There are various ways this setup can be improved. Rather than broadcasting assignments, each agent could, instead, broadcast optimistic scores for certain assignments, and receive messages from neighboring agents dictating if such assignments are possible. If they are not, then backtracking must be done. This is still asymptotically exponential, but is guided in a smarter way; being best-first instead of depth-first.

These search-based methods can benefit heavily from preprocessing, such as the decomposition of isolated parts; see sections \ref{sec:vertex-splitting} and \ref{sec:csp-decomposition}. This would allow parts of the search to be performed in parallel in relative isolation; though this possibility is heavily problem-dependent. There are many useful preprocessing techniques, and this would require a centralized preprocessor.

There are also algorithms based, not on search, but on inference. These will calculate a representation of all good options; essentially a tree representing each value combination, pruned for redundancy. This entire database is propagated to other agents, so they may further prune and add to the tree based on these new agents' options. Ultimately, this propagates up to the head agent, which makes a final pruning which is the optimal choice. In this setup, we no longer have an exponential number of messages but, instead, a linear number of messages exponential in size. Each agent is also expected to do an exponential amount of work.

These exponential sizes are intrinsic if we want to guarantee some kind of optimum. In many cases, we cannot afford such an effort, and so must accept a best-effort suboptimal solution.

\subsection{Incomplete Algorithms}\label{sec:incomplete-algorithms}

Incomplete algorithms are more varied and often simpler than their complete counterparts. One fundamental approach involves agents performing localized hill-climbing, where they periodically update their variables to an optimal value. This local optimization can be further enriched through inter-agent communication, allowing the agents to consider assignment costs that affect multiple participants. A major challenge to CSP decomposition is the inability to combine optimal solutions to sub-problems to get an optimal solution for the entire problem. Despite this, for practical applications, combining these sub-solutions often yields results that are good enough. What local context is being optimized can vary. Rather than optimizing between actual choices made by neighboring agents, they may optimize against some expectation of values determined by some heuristic, or through a distribution propagated by other agents.

Agents can be given additional powers, allowing them to more smartly decide on good solutions. A solution might look locally good but would cause other agents to react in such a way that it becomes a bad choice. As such, each choice is a gamble, of sorts, where the immediate change in value is merely a hint. Agents can, instead, run bandit algorithms \citep{lattimore2020bandit} to make better decisions over time.

Perhaps the most relevant incomplete method for our application are stochastic search methods. The most classic method is WalkSAT. This algorithm involves looking up an unsatisfied clause, and randomly changing a variable to make that clause satisfied. This is then repeated until either satisfaction or some timeout. While the original version of the algorithm was sequential, there are many simple ways to make the algorithm more parallel, as well as adaptations of more sophisticated techniques from other approaches to solving \citep{mcdonald2009parallel}.

More promising for general combinatorial optimization are the so-called annealing methods. These come in many flavors, but the idea is to replace discrete variables with continuous variables ranging between real values encoding possible assignments. The problem is then turned into a description for a potential well where solutions are global minima and better solutions are lower minima. The values are then evolved according to a stochastic differential equation of some kind in the hopes of landing at a decent minimum.

As a basic example, we may model the inequality relation in three coloring by the following 2-dimensional potential;

\begin{equation}
    V(x, y) := \prod_{i, j \in \{-1, 0, 1\}, i \neq j} (x - i)^2 + (y - j)^2
\end{equation}

Here, the three colors are encoded as -1, 0, and 1, and this potential reaches a global minimum of 0 if and only if the input pair encodes an unequal pair of colors. Given a CSP, we can convert each constraint into a potential, and then generate a potential for the whole problem by summing all the constraint potentials. The most basic way to turn this into a dynamical system is to interpret the potential as a gradient flow. We can simulate a particle following the equation $x_i'(t) = -\partial_{x_i(t)} V(x(t))$, for each dimension/variable $i$. This is just gradient descent. Generally, this method is prone to get stuck in local minima, but there is a vast body of research, mainly coming from machine learning, for improving this method \citep{ruder2016overview}.

Much can be done to improve things like numerical stability and modeling through careful consideration of the potential. The potential we have given as an example is easy to understand, but suboptimal for most purposes. One can replace the squares with absolute values and the product with taking a minimum, and one will get a linearized "triangle" potential with minima at the same points. Furthermore, by restricting the domain, say to $x, y \in [-1, 1]$, the potential will be bounded, and we may then normalize it to $[0, 1]$; appropriate as these model propositions. This assists in further generalizations. With normalized potentials, we can consistently model weighted MaxCSP by simply multiplying each potential by their weights when summing the potentials for each clause. We may then handle hard constraints by setting their weight to be greater than the sum of the weights of the soft constraints. This ensures that every hard clause can overpower all the soft clauses if need be.

There are two classic annealing methods used to minimize these potentials; thermal annealing and chaotic annealing. In thermal annealing, the trajectory of assignments receives a force directly from the potential well assigned to the problem, plus an additional kick from random, usually white, noise \citep{kirkpatrick1983optimization}. More technically, this would involve simulating something like $x_i'(t) = -\partial_{x_i(t)} V(x(t)) + W(t)$, where $W(t)$ is a random process (usually a Gaussian) sampled at each time-step of the simulation. So long as the temperature/size of kicks decreases over time slowly enough (often exponentially slow), one is essentially guaranteed to settle in a global minimum. Chaotic annealing is another extreme where the dynamic system is entirely deterministic, but chaotic dynamics are introduced to facilitate exploration. Generally, a chaotic function is treated as noise added to the system. So the dynamics might look something like $x_i'(t) = -\partial_{x_i(t)} V(x(t)) + g(t)$, where $g(t)$ is a chaotic function replacing nondeterministic noise. Temperature becomes a coefficient on this function when added to the potential, so the chaos has less of an influence the lower the temperature. The key property of chaotic dynamics is that the same point is never explored twice, accelerating a search for minima \citep{zhou1997chaotic}.

These two methods have different drawbacks. Thermal annealing is very simple, but we have poor guarantees for the rate of convergence. Chaotic annealing can give better guarantees, but the cost of simulating a chaotic system can grow exponentially with the need for accuracy. Chaotic systems are also often quite sensitive to noise, making hardware implementations of such systems less reliable than their theoretical counterparts. More recent work in the theory of stochastics (see \citep{ovchinnikov2016introduction}) has led to the creation of so-called "N-phase" annealing that compromises between the two. Exactly how these should be implemented is still an ongoing project, but there seem to be two main descriptions. In \citep{ovchinnikov2017stochastic}, N-phase annealing is described as continuously shifting between a near-thermal phase and a near-chaotic phase based on how long its been since progress has been made on optimization. Without a practical implementation, it is hard to know exactly what this would look like. Another approach is described in \citep{di2022memcomputing}, where the system is entirely deterministic, and stochastics are assumed to be natural environmental noise to which the system must be robust. The system is non-chaotic but periodically enters critical states that upset the global configuration of the system until a better minimum is found. This is done by adding, for each clause, two variables, one being a "short-term" memory indicating the degree to which the constraint was satisfied in the near-past, and a "long-term" memory, increasing the potential generated by the associated clause the longer it is remained unsatisfied. Schematically, the set of differential equations will be some variant of the following;

\begin{equation}
    x_i'(t) = \sum_{m \in \text{Clauses}} -\partial_{x_i(t)} V_m(x(t)) \big(s_m(t) l_m(t) + (1 - s_m(t)) (1 + \zeta l_m(t)) R_{i, m}\big)
\end{equation}

\begin{equation}
    s_i'(t) = \beta (s_i(t) + \epsilon) (V_i(x(t)) - \gamma)\ \quad \quad \quad\ l_i'(t) = \alpha (V_i(x(t)) - \delta)
\end{equation}

Where $s_i(t) \in [0, 1]$, $l_i(t) \geq 0$, the Greek letters are constant hyperparameters that must be tuned, and $R_{i,m}$ is a function assigning a value in $[0, 1]$ indicating the degree to which variable $i$ is responsible for clause $m$ being satisfied. Conceptually, the gradient of each variable descends the potential but is modulated by the short and long-term memory. If the short-term memory is 1 (i.e. not satisfied) the long-term memory is in full force, but if it is 0 (i.e. satisfied), the long-term memory is depressed, and, further, a variable is only affected by the potential of a satisfied clause if that variable is the reason the clause is satisfied (i.e. if $R$ is 1). In our example of three coloring, $R$ will always be 1, but this is not the case in, for example, 3-SAT where a single variable can cause a clause to be satisfied irrespective of the other variables in that clause. See, for example, \citep{bearden2020efficient} and \citep{zhang2023implementation} for an explicit example in the context of boolean sat and details on setting the constants. 

When the potential of a long-time unsatisfied clause gets powerful enough to overcome the potentials of the clauses that share variables with it, this triggers a critical transition. With particular variants, the dynamical systems can be made to be dissipative; that is, all volumes decrease over time. This guarantees that the system will eventually settle into a well around some stable equilibria, as this prevents cycles or chaos. In such a case, one can understand the new system as turning all local minima in the original potential into saddle points. Such methods have been specialized for integer programming in \citep{traversa2018memcomputing}. Such methods have recently shown empirical quadratic scaling for prime factorization on classical computers \citep{sharp2023scaling}. Existing designs are hand-crafted using expert knowledge, but see \citep{caravelli2021global} and \citep{caravelli2023projective} for recent work on more systematic transformations of dynamical systems. These methods echo some previous physics-inspired methods, such as the method of discrete Lagrange multipliers; see \citep{shang1998discrete} and section 6.3 of \citep{biere2009handbook}, and survey propagation; see \citep{mezard2002analytic} and sections 6.5 and 10.8 of \citep{biere2009handbook}.